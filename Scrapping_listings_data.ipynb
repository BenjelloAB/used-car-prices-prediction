{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512eb630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping the posts :\n",
    "#  well we used the workers to execute the fetching in parallel\n",
    "#  retry decorators are used to treat the 429 error explain it too\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from google.colab import files\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2137164c",
   "metadata": {},
   "source": [
    "Ce dictionnaire (brands) contient les marques de véhicules qui ont été extraites dynamiquement du site web source. Elles ont été collectées à l'aide du script de web scraping utilisant Selenium, en interagissant avec des listes déroulantes ou d'autres éléments interactifs de la page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b1a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "brands = {\n",
    "    'Mercedes-Benz': 41, 'Renault': 49,\n",
    "    'Peugeot': 46, 'Toyota': 56, 'Ford': 18, 'Volkswagen': 58, 'Fiat': 17,\n",
    "    'Hyundai': 24, 'Nissan': 44, 'Chevrolet': 10, 'Kia': 30, 'Citroen': 12,\n",
    "    'Mazda': 40, 'Opel': 45, 'Suzuki': 55, 'BMW': 5, 'Honda': 22,\n",
    "    'Audi': 3, 'Alfa Romeo': 1, 'Seat': 50, 'Lada': 72, 'Cadillac': 7,\n",
    "    'Volvo': 59, 'DFSK': 67, 'Jeep': 29, 'Chery': 9, 'Porsche': 48,\n",
    "    'Skoda': 51, 'Daewoo': 61, 'Jaguar': 28, 'Rover': 62, 'Ssangyong': 53,\n",
    "    'Geely': 20, 'Mitsubishi': 43, 'Dacia': 13\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b1f99d",
   "metadata": {},
   "source": [
    "une technique pour simuler des connexions depuis différents navigateurs réels, afin de réduire le risque d'être détecté et bloqué par le site web source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fee023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User agents for rotation\n",
    "user_agents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85e74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=5, min=5, max=20),\n",
    "    retry=retry_if_exception_type(requests.exceptions.HTTPError),\n",
    "    before_sleep=lambda retry_state: print(f\"429 error in collect_links, retrying in {retry_state.next_action.sleep} seconds...\")\n",
    ")\n",
    "def collect_links(brand_id, max_pages=30):\n",
    "    \"\"\"Collect post links for a given brand using requests.\"\"\"\n",
    "    base_url = f\"https://www.avito.ma/fr/maroc/voitures_d_occasion-%C3%A0_vendre?has_price=true&sp=1&brand={brand_id}\"\n",
    "    links = set()\n",
    "    headers = {'User-Agent': random.choice(user_agents)}\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        url = f\"{base_url}&o={page}\"\n",
    "        try:\n",
    "            print(f\"Fetching {url}\")\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            listings = soup.select('div.sc-1nre5ec-1.crKvIr.listing > a')\n",
    "            page_links = [listing['href'] for listing in listings if listing.get('href')]\n",
    "\n",
    "            if not page_links:\n",
    "                print(f\"No links found on page {page} for brand {brand_id}, stopping.\")\n",
    "                break\n",
    "            elif len(page_links) < 5 and page > 1:\n",
    "                print(f\"Only {len(page_links)} links found on page {page} for brand {brand_id}, stopping early.\")\n",
    "                links.update(page_links)\n",
    "                break\n",
    "\n",
    "            links.update(page_links)\n",
    "            print(f\"Collected {len(page_links)} links from page {page} for brand {brand_id}\")\n",
    "            time.sleep(random.uniform(5, 10))  # Increased delay\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "              print(f\"Failed to fetch {url}: {e}\")\n",
    "              break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {url}: {e}\")\n",
    "            break\n",
    "\n",
    "    return list(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a13bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry(\n",
    "    stop=stop_after_attempt(3),\n",
    "    wait=wait_exponential(multiplier=5, min=5, max=20),\n",
    "    retry=retry_if_exception_type(requests.exceptions.HTTPError),\n",
    "    before_sleep=lambda retry_state: print(f\"429 error, retrying in {retry_state.next_action.sleep} seconds...\")\n",
    ")\n",
    "def scrape_detail_page(url, brand_id, brand_name):\n",
    "    \"\"\"Scrape a single post's detail page using requests.\"\"\"\n",
    "    headers = {'User-Agent': random.choice(user_agents)}\n",
    "    response = requests.get(url, headers=headers, timeout=15)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    def get_text(selector):\n",
    "        element = soup.select_one(selector)\n",
    "        return element.text.strip() if element else 'N/A'\n",
    "\n",
    "    data = {\n",
    "        'url': url,\n",
    "        'title': get_text('div.sc-1veij0r-3.cKDbCG > h1'),\n",
    "        'price': get_text('div.sc-1g3sn3w-1.iIzVCI > div.sc-1gfa0w0-0.gQtXSS > div:nth-child(2) > div.sc-1lz4h6h-0.jRkCWq > div.sc-1g3sn3w-0.cgzdaK > div.sc-1g3sn3w-4.etbZjx > div.sc-1veij0r-0.bDmIpE > div > div.sc-1veij0r-7.jjDooz > div > p'),\n",
    "        'location': get_text('div.sc-1veij0r-3.cKDbCG > div > span:nth-child(2)'),\n",
    "        'brand': brand_name\n",
    "    }\n",
    "\n",
    "    feature_container = soup.select_one('div.sc-1g3sn3w-4.etbZjx > div:nth-child(5) > div')\n",
    "    features = {\n",
    "        'Année-Modèle': 'year',\n",
    "        'Boite de vitesses': 'transmission',\n",
    "        'Type de carburant': 'fuel_type',\n",
    "        'Kilométrage': 'kilometrage',\n",
    "        'Marque': 'brand_check',\n",
    "        'Modèle': 'submodel',\n",
    "        'Nombre de portes': 'door_number',\n",
    "        'Origine': 'origin',\n",
    "        'Première main': 'first_owner',\n",
    "        'État': 'condition',\n",
    "        'Puissance fiscale': 'puissance_fiscale'\n",
    "    }\n",
    "\n",
    "    if feature_container:\n",
    "        feature_divs = feature_container.select('div.sc-19cngu6-2.kuofIS')\n",
    "        for div in feature_divs:\n",
    "            title_span = div.select_one('span.sc-1x0vz2r-0.bXFCIH')\n",
    "            value_span = div.select_one('span.sc-1x0vz2r-0.fjZBup')\n",
    "            if title_span and value_span:\n",
    "                title = title_span.text.strip()\n",
    "                value = value_span.text.strip()\n",
    "                if title in features:\n",
    "                    data[features[title]] = value\n",
    "\n",
    "    extra_container = soup.select_one('div.sc-1g3sn3w-4.etbZjx > div:nth-child(8) > div')\n",
    "    extra_features = [\n",
    "        'ABS', 'Airbags', 'Vitres électriques', 'Verrouillage centralisé à distance',\n",
    "        'Radar de recul', 'Système de navigation/GPS', 'Caméra de recul',\n",
    "        'Limiteur de vitesse', 'Jantes aluminium', 'ESP', 'Climatisation',\n",
    "        'CD/MP3/Bluetooth', 'Sièges cuir', 'Ordinateur de bord', 'Toit ouvrant',\n",
    "        'Régulateur de vitesse'\n",
    "    ]\n",
    "    for feature in extra_features:\n",
    "        data[feature.lower().replace(' ', '_')] = False\n",
    "\n",
    "    if extra_container:\n",
    "        extra_spans = extra_container.select('div > div > div > span')\n",
    "        for span in extra_spans:\n",
    "            feature_name = span.text.strip()\n",
    "            if feature_name in extra_features:\n",
    "                data[feature_name.lower().replace(' ', '_')] = True\n",
    "\n",
    "    time.sleep(random.uniform(5, 10))  # Increased delay\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae2d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_brand(brand_id, brand_name, max_pages=30):\n",
    "    \"\"\"Scrape all posts for a given brand with parallel detail scraping.\"\"\"\n",
    "    print(f\"Scraping brand {brand_name} (ID: {brand_id})\")\n",
    "    links = collect_links(brand_id, max_pages)\n",
    "    print(f\"Collected {len(links)} links for brand {brand_name}\")\n",
    "\n",
    "    data = []\n",
    "    def scrape_link(link):\n",
    "        try:\n",
    "            print(f\"[{brand_name}] Scraping detail page {links.index(link) + 1}/{len(links)}: {link}\")\n",
    "            return scrape_detail_page(link, brand_id, brand_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {link}: {e}\")\n",
    "            return None\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # Reduced to 2 detail workers\n",
    "        results = executor.map(scrape_link, links)\n",
    "        for post_data in results:\n",
    "            if post_data:\n",
    "                data.append(post_data)\n",
    "\n",
    "    if data:\n",
    "        # df = pd.DataFrame(data)\n",
    "        # df.to_csv(f'avito_brand_{brand_id}_temp.csv', index=False)\n",
    "        # print(f\"[{brand_name}] Incremental save: {len(data)} posts for brand {brand_id}\")\n",
    "        df = pd.DataFrame(data)\n",
    "        temp_file = f'avito_brand_{brand_id}_temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "        print(f\"[{brand_name}] Incremental save: {len(data)} posts to {temp_file}\")\n",
    "        if os.path.exists(temp_file):\n",
    "            print(f\"Downloading {temp_file}\")\n",
    "            files.download(temp_file)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3438056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    output_path = \"/content/avito_car_listings_reduced_workers.csv\"\n",
    "    all_data = []\n",
    "    visited_links = set()\n",
    "    target_posts = 70000\n",
    "\n",
    "    # sample_brands = dict(list(brands.items())[:1])\n",
    "\n",
    "\n",
    "    print(\"Starting parallel scraping for brands...\")\n",
    "    # with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        brand_results = executor.map(\n",
    "            lambda brand: scrape_brand(brand[1], brand[0], max_pages=30),\n",
    "            brands.items()\n",
    "        )\n",
    "        for brand_data in brand_results:\n",
    "            for post in brand_data:\n",
    "                if post and post['url'] not in visited_links:\n",
    "                    all_data.append(post)\n",
    "                    visited_links.add(post['url'])\n",
    "                    print(f\"Global progress: {len(all_data)}/{target_posts} posts collected\")\n",
    "\n",
    "            if len(all_data) >= 1000:\n",
    "                df = pd.DataFrame(all_data)\n",
    "                df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "                print(f\"Saved {len(df)} posts to {output_path}\")\n",
    "\n",
    "            if len(all_data) >= target_posts:\n",
    "                print(f\"Reached target of ~{target_posts} posts\")\n",
    "                break\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df = df.drop_duplicates(subset=['url', 'title', 'price', 'location'])\n",
    "    columns = [\n",
    "        'url', 'title', 'price', 'location', 'brand', 'submodel', 'year',\n",
    "        'transmission', 'fuel_type', 'kilometrage', 'door_number', 'origin',\n",
    "        'first_owner', 'condition', 'puissance_fiscale', 'abs', 'airbags',\n",
    "        'vitres_électriques', 'verrouillage_centralisé_à_distance', 'radar_de_recul',\n",
    "        'système_de_navigation/gps', 'caméra_de_recul', 'limiteur_de_vitesse',\n",
    "        'jantes_aluminium', 'esp', 'climatisation', 'cd/mp3/bluetooth',\n",
    "        'sièges_cuir', 'ordinateur_de_bord', 'toit_ouvrant', 'régulateur_de_vitesse'\n",
    "    ]\n",
    "    df = df.reindex(columns=columns, fill_value='N/A')\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    print(f\"Final save: {len(df)} unique posts to {output_path}\")\n",
    "    files.download(output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
